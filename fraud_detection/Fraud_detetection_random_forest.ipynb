{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard = pd.read_csv('creditcard_sampledata_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7300 entries, 0 to 7299\n",
      "Data columns (total 31 columns):\n",
      "Unnamed: 0    7300 non-null int64\n",
      "V1            7300 non-null float64\n",
      "V2            7300 non-null float64\n",
      "V3            7300 non-null float64\n",
      "V4            7300 non-null float64\n",
      "V5            7300 non-null float64\n",
      "V6            7300 non-null float64\n",
      "V7            7300 non-null float64\n",
      "V8            7300 non-null float64\n",
      "V9            7300 non-null float64\n",
      "V10           7300 non-null float64\n",
      "V11           7300 non-null float64\n",
      "V12           7300 non-null float64\n",
      "V13           7300 non-null float64\n",
      "V14           7300 non-null float64\n",
      "V15           7300 non-null float64\n",
      "V16           7300 non-null float64\n",
      "V17           7300 non-null float64\n",
      "V18           7300 non-null float64\n",
      "V19           7300 non-null float64\n",
      "V20           7300 non-null float64\n",
      "V21           7300 non-null float64\n",
      "V22           7300 non-null float64\n",
      "V23           7300 non-null float64\n",
      "V24           7300 non-null float64\n",
      "V25           7300 non-null float64\n",
      "V26           7300 non-null float64\n",
      "V27           7300 non-null float64\n",
      "V28           7300 non-null float64\n",
      "Amount        7300 non-null float64\n",
      "Class         7300 non-null int64\n",
      "dtypes: float64(29), int64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "creditcard.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the Data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(dataframe):\n",
    "    y = dataframe.Class.values\n",
    "    X = dataframe.drop(['Unnamed: 0' , 'V1', 'Class'] , axis = 1).values\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = prep_data(creditcard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len =len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fraud_count = len([i for i in y if i==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_percent=non_fraud_count/total_len*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.8904109589041"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to perform better than this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance of BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report , roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2096,    3],\n",
       "       [  18,   73]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      2114\n",
      "           1       0.80      0.96      0.87        76\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2190\n",
      "   macro avg       0.90      0.98      0.93      2190\n",
      "weighted avg       0.99      0.99      0.99      2190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9338879319822626"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, model.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904109589041096"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Parameters of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjusting Wieght to deal with Heavily Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_weight=\n",
    "**{balanced mode:** Uses value of `y` to automatically adjust wieghts inversely proportional to class frequencies in input data,\n",
    "\n",
    "**balanced_subsample:** same as balanced except weigths are calculated `again` at each iteration of a growing tree in a random forest}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of trees: `n_estimators` <br>\n",
    "number of Features considered for splitting at each leaf node : `max_features` <br>\n",
    "shape of the Tree: leaf_size<br>\n",
    "length of tree: max_depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning using GridSearchCV \n",
    "example:\n",
    "<br>\n",
    "from sklearn.model_selection import `GridSearchCV`\n",
    "\n",
    "param_grid = {<br>\n",
    "                'max_depth':[90,90,100]<br>\n",
    "                'max_features':[2,3]<br>\n",
    "                'n_estimators':[100,200]<br>\n",
    "                }\n",
    "                \n",
    "                \n",
    "                \n",
    "model = `RandomForestRegressor()` <br>\n",
    "grid_search_model = GridSearchCV(estimator = model, param_grid = `param_grid1` , cv = 5 , n_jobs = -1 , scoring = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Grid object on data**\n",
    "grid_search_model.fit(X_train, y_train) <br>\n",
    "grid_search_model.best_params_ <br>\n",
    "grid_search_model.best_estimator_<br>\n",
    "grid_search_model.best_score_<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Create Model2 with class_weight = 'balanced_subsample' option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier(class_weight='balanced_subsample', random_state = 5)\n",
    "model2.fit(X_train,y_train)\n",
    "y_pred=model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2098,    1],\n",
       "       [  19,   72]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2099\n",
      "           1       0.99      0.79      0.88        91\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2190\n",
      "   macro avg       0.99      0.90      0.94      2190\n",
      "weighted avg       0.99      0.99      0.99      2190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9338879319822626"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, model.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908675799086758"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Write a function to Print Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create One more Model by adjusting other parameters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3  = RandomForestClassifier(bootstrap = True , class_weight = {0:1 , 1:12 } , criterion = 'entropy' , \n",
    "                                #depth of Model\n",
    "                                max_depth = 10,\n",
    "                                #minimum number of samples at a leaf\n",
    "                                min_samples_leaf=10 ,  \n",
    "                                #number of trees \n",
    "                                n_estimators=20 , n_jobs = -1 , random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2097    2]\n",
      " [  14   77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2099\n",
      "           1       0.97      0.85      0.91        91\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2190\n",
      "   macro avg       0.98      0.92      0.95      2190\n",
      "weighted avg       0.99      0.99      0.99      2190\n",
      "\n",
      "0.9609651901219315\n"
     ]
    }
   ],
   "source": [
    "get_score(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908675799086758"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the ModelScore has improved.\n",
    "<br>Now let's Use GridSearchCV to find optimal Parameters and create final Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimenting GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'n_estimators':[1,30],\n",
    "          'max_features': ['auto', 'log2'] , \n",
    "          'max_depth' : [4,8] , \n",
    "          'criterion' : ['gini', 'entropy']\n",
    "            }\n",
    "\n",
    "sample_model = RandomForestClassifier(random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = GridSearchCV(estimator= sample_model, param_grid=params,cv = 5 , scoring='recall' , n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=5, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_estimators': [1, 30], 'max_features': ['auto', 'log2'], 'max_depth': [4, 8], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='recall', verbose=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 8,\n",
       " 'max_features': 'log2',\n",
       " 'n_estimators': 30}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Create the Final Model using parameter's suggested by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = RandomForestClassifier(random_state = 5 , criterion='gini', max_depth = 8 ,max_features = 'log2' , n_estimators =30 ,\n",
    "                                 class_weight = {0:1 , 1 :12} , n_jobs = -1, min_samples_leaf =10\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2095    4]\n",
      " [  15   76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2099\n",
      "           1       0.95      0.84      0.89        91\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2190\n",
      "   macro avg       0.97      0.92      0.94      2190\n",
      "weighted avg       0.99      0.99      0.99      2190\n",
      "\n",
      "0.9749697658225529\n"
     ]
    }
   ],
   "source": [
    "get_score(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908675799086758"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Proceed with Using Ensemble Method to create ML Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Methods Combine the result of Mutiple Models to predict the result\n",
    "Infact Random Forest is also an ensemble of Decision tree and thus known as Bootstrap Aggrgation or Bagging Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bagging Ensemble` : Models are Trained on Random subset of Data, and aggregate result by taking average prediction of each model. <br>\n",
    "`Stacking Ensemble` : Models are trained on Full set of Data,results are predicted by a Voting rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Methods are:\n",
    "    - Robust\n",
    "    - Avoid Overfitting\n",
    "    - Improve prediction Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: <br>\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier <br>\n",
    "clf1 = LogisticRegression(random_state = 5) <br>\n",
    "clf2 = RandomForestClassifier(random_state = 5 ) <br>\n",
    "clf3 = GaussianNB() <br>\n",
    "\n",
    "ens_model = VotingClassifier(estimators= [('lr' , clf1) , ('rf' , clf2) , ('nb' , clf3)] , voting = 'hard') <br>\n",
    "ens_model.fit(X_train, y_train) <br>\n",
    "ens_model.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Rule Parameter \n",
    "`Hard` : Uses predicted class Labels and takes **Majority Votes**. <br>\n",
    "`Soft` : uses **Average probability** by combining probablities of individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can Also assign weights to model prediction when we know that one of the models is outperforming. <br>\n",
    "`ens_model` = VotingClassifier( estimators = [('lr' , clf1) ,( 'rf', clf2),( 'nb', clf3) ], voting = 'hard' , weights = [1,2,1] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's impelment this on our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2052   47]\n",
      " [  11   80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      2099\n",
      "           1       0.63      0.88      0.73        91\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2190\n",
      "   macro avg       0.81      0.93      0.86      2190\n",
      "weighted avg       0.98      0.97      0.98      2190\n",
      "\n",
      "0.9722054981702433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(random_state = 5 , class_weight = {0:1 , 1 :15})\n",
    "get_score(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908675799086758"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling by stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state = 5 , class_weight = {0:1 , 1 :15})\n",
    "clf2 = RandomForestClassifier( random_state = 5 , criterion='gini', max_depth = 8 ,max_features = 'log2' , n_estimators =30 ,\n",
    "                                 class_weight = {0:1 , 1 :12} , n_jobs = -1, min_samples_leaf =10   )\n",
    "clf3 = DecisionTreeClassifier(random_state = 5 , class_weight = 'balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = VotingClassifier(estimators = [('lr' , clf1) , ('rf' , clf2), ('dt' , clf3)], voting ='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScore_hard(ens_model):\n",
    "    ens_model.fit(X_train,y_train)\n",
    "    y_pred = ens_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2090    9]\n",
      " [  13   78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2099\n",
      "           1       0.90      0.86      0.88        91\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2190\n",
      "   macro avg       0.95      0.93      0.94      2190\n",
      "weighted avg       0.99      0.99      0.99      2190\n",
      "\n",
      "0.9899543378995433\n"
     ]
    }
   ],
   "source": [
    "GetScore_hard(ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add Weights to Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model_weighed = VotingClassifier(estimators  = [('lr', clf1) , ('rf' , clf2) , ('dt', clf3)], weights = [1,4,1], voting = 'hard',  flatten_transform = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2095    4]\n",
      " [  15   76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2099\n",
      "           1       0.95      0.84      0.89        91\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2190\n",
      "   macro avg       0.97      0.92      0.94      2190\n",
      "weighted avg       0.99      0.99      0.99      2190\n",
      "\n",
      "0.991324200913242\n"
     ]
    }
   ],
   "source": [
    "GetScore_hard(ensemble_model_weighed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
